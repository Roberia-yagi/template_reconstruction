{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import GPUtil\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import tqdm \n",
    "import datetime\n",
    "from typing import Any, Union, Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from util import save_json, load_json, create_logger, resolve_path\n",
    "from fairface import Fairface\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_options() -> Any:\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Timestamp\n",
    "    time_stamp = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "    parser.add_argument(\"--identifier\", type=str, default=time_stamp, help=\"name of result folder\")\n",
    "\n",
    "    # Folders\n",
    "    parser.add_argument(\"--save\", type=bool, default=False, help=\"if save log and data\")\n",
    "    parser.add_argument(\"--dataset_dir\", type=str, default=\"~/nas/dataset/fairface\", help=\"path to directory which includes dataset(Fairface)\")\n",
    "    parser.add_argument(\"--result_dir\", type=str, default=\"~/nas/results/attribute_inversion/step1\", help=\"path to directory which includes results\")\n",
    "\n",
    "    # Conditions\n",
    "    parser.add_argument(\"--gpu_idx\", type=int, default=0, help=\"index of cuda devices\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "    parser.add_argument(\"--img_channels\", type=int, default=3, help=\"number of image channels\")\n",
    "    parser.add_argument(\"--img_size\", type=int, default=64, help=\"size of each image dimension\")\n",
    "    parser.add_argument(\"--data_num\", type=int, default=10000, help=\"number of data to use\")\n",
    "    parser.add_argument(\"--attack_model\", type=str, default=\"ResNet152\", help=\"attack model(feature extraction): 'VGG16', 'ResNet152', 'FaceNet'\")\n",
    "    parser.add_argument(\"--target_model\", type=str, default=\"FaceNet\", help=\"target model(feature extraction): 'VGG16', 'ResNet152', 'FaceNet'\")\n",
    "    parser.add_argument(\"--attributes\", nargs='+', help=\"attributes to slice the dataset(age, gender, race)\")\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model: str) -> Tuple[nn.Module, int, str]:\n",
    "    layer_dict = dict()\n",
    "    img_size_resnet152 = 112\n",
    "    img_size_facenet = 160\n",
    "\n",
    "    if model == \"ResNet152\":\n",
    "        layer_name = \"flatten\"\n",
    "        model = models.resnet152(pretrained=True) # pre-trained on ImageNet\n",
    "        layer_dict[layer_name] = layer_name\n",
    "        model = create_feature_extractor(model, layer_dict)\n",
    "        return model, img_size_resnet152, layer_name\n",
    "    \n",
    "    if model == \"FaceNet\":\n",
    "        return InceptionResnetV1(classify=False, num_classes=None, pretrained=\"vggface2\"), img_size_facenet, None\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_fairface_dataset_loader(\n",
    "    base_dir: str,\n",
    "    usage: str,\n",
    "    data_num: int,\n",
    "    attribute_group_list: dict,\n",
    "    options: Any,\n",
    ") -> Tuple[Fairface, list]:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    attribute_list = []\n",
    "\n",
    "    for attribute_group in attribute_group_list:\n",
    "        if attribute_group in options.attributes:\n",
    "            attribute_list.append(attribute_group_list[attribute_group])\n",
    "\n",
    "    attributes_list = list(itertools.product(*attribute_list))\n",
    "\n",
    "    datasets = dict()\n",
    "    dataloaders = dict()\n",
    "\n",
    "    for attributes in attributes_list:\n",
    "        dataset = Fairface(\n",
    "            base_dir=base_dir,\n",
    "            usage=usage,\n",
    "            transform=transform,\n",
    "            data_num=data_num,\n",
    "            attributes=attributes\n",
    "        )\n",
    "\n",
    "        datasets[attributes] = dataset\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=options.batch_size,\n",
    "            shuffle=False,\n",
    "            # Optimization:\n",
    "            num_workers=os.cpu_count(),\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        dataloaders[attributes] = dataloader\n",
    "\n",
    "    return datasets, dataloaders, attributes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_feature(batch: Any,\n",
    "                    device: Any,\n",
    "                    transform: transforms,\n",
    "                    model: nn.Module,\n",
    "                    layer_name:str):\n",
    "    batch= transform(batch[0]).to(device)\n",
    "    feature = model(batch)\n",
    "    if type(feature) is dict:\n",
    "        feature = feature[layer_name]\n",
    "\n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_umap(features_T: torch.Tensor,\n",
    "                   features_F: torch.Tensor,\n",
    "                   attribute_datanum: dict,\n",
    "                   options: Any):\n",
    "    # UMAP\n",
    "    embedding_T = umap.UMAP().fit_transform(features_T.detach().cpu())\n",
    "    embedding_F = umap.UMAP().fit_transform(features_F.detach().cpu())\n",
    "    umap_x_T=embedding_T[:,0]\n",
    "    umap_y_T=embedding_T[:,1]\n",
    "    umap_x_F=embedding_F[:,0]\n",
    "    umap_y_F=embedding_F[:,1]\n",
    "    base_datanum = 0\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, sharex=False)\n",
    "    for attribute in attribute_datanum:\n",
    "        datanum = attribute_datanum[attribute]\n",
    "        axes[0].scatter((np.mean(umap_x_T[base_datanum:base_datanum+datanum])), np.mean(umap_y_T[base_datanum:base_datanum+datanum]), label=(''.join(attribute) + \"_T\"))\n",
    "        axes[1].scatter(np.mean(umap_x_F[base_datanum:base_datanum+datanum]), np.mean(umap_y_F[base_datanum:base_datanum+datanum]), label=(''.join(attribute) + \"_F\"))\n",
    "        base_datanum += datanum\n",
    "    axes[0].legend(prop={'size': 5})\n",
    "    axes[1].legend(prop={'size': 5})\n",
    "    plt.title(\"UMAP\")\n",
    "    plt.savefig(resolve_path(options.result_dir, 'umap.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "options = get_options()\n",
    "\n",
    "# Decide device\n",
    "device = f\"cuda:{options.gpu_idx}\" if torch.cuda.is_available() else \"cpu\"\n",
    "options.device = device\n",
    "\n",
    "# Create directories to save data\n",
    "if options.save:\n",
    "    # Create directory to save results\n",
    "    result_dir = resolve_path(options.result_dir, options.identifier)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # Save step1 options\n",
    "    save_json(resolve_path(result_dir, \"step1.json\"), vars(options))\n",
    "\n",
    "# Create logger\n",
    "if options.save:\n",
    "    logger = create_logger(f\"Step 1\", resolve_path(result_dir, \"training.log\"))\n",
    "else:\n",
    "    logger = create_logger(f\"Step 1\")\n",
    "\n",
    "# Log options\n",
    "logger.info(vars(options))\n",
    "\n",
    "# Load attribute list\n",
    "attribute_group_list = load_json(\"attributes.json\")\n",
    "\n",
    "T, img_size_T, _ = load_model(options.target_model)\n",
    "F, img_size_F, layer_name_F = load_model(options.attack_model)\n",
    "T.to(device)\n",
    "F.to(device)\n",
    "T.eval()\n",
    "F.eval()\n",
    "\n",
    "datasets, dataloaders, attributes_list = load_fairface_dataset_loader(options.dataset_dir, \\\n",
    "                                            'train', \\\n",
    "                                            options.data_num, \\\n",
    "                                            attribute_group_list, \\\n",
    "                                            options)\n",
    "\n",
    "transform_T = transforms.Compose([\n",
    "    transforms.Resize((img_size_T, img_size_T)),\n",
    "])\n",
    "transform_F = transforms.Compose([\n",
    "    transforms.Resize((img_size_F, img_size_F)),\n",
    "])\n",
    "\n",
    "attribute_feature_T = dict()\n",
    "attribute_feature_F = dict()\n",
    "for dataloader_name in tqdm(dataloaders):\n",
    "    features_T = torch.Tensor().to(device)\n",
    "    features_F = torch.Tensor().to(device)\n",
    "    # if 'Black' in dataloader_name or 'White' in dataloader_name or 'East Asian' in dataloader_name:\n",
    "    for batch in dataloaders[dataloader_name]:\n",
    "        feature_T = extract_feature(batch, device, transform_T, T, None)\n",
    "        feature_F = extract_feature(batch, device, transform_F, F, layer_name_F)\n",
    "        features_T = torch.cat((features_T, feature_T.detach()), 0)\n",
    "        features_F = torch.cat((features_F, feature_F.detach()), 0)\n",
    "    attribute_feature_T[dataloader_name] = features_T\n",
    "    attribute_feature_F[dataloader_name] = features_F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for attribute in attribute_feature_T:\n",
    "    sum_of_cosine_similarity_T = 0\n",
    "    sum_of_cosine_similarity_F = 0\n",
    "    features_T = attribute_feature_T[attribute]\n",
    "    features_F = attribute_feature_F[attribute]\n",
    "    average_of_features_T = torch.mean(features_T, dim=0)\n",
    "    average_of_features_F = torch.mean(features_F, dim=0)\n",
    "    print(average_of_features_T)\n",
    "    print(average_of_features_T.shape)\n",
    "    average_of_features_T = average_of_features_T.expand(features_T.shape[0], -1)\n",
    "    average_of_features_F = average_of_features_F.expand(features_F.shape[0], -1)\n",
    "    print(average_of_features_T.shape)\n",
    "    metric = nn.CosineSimilarity(dim=1)\n",
    "    sum_of_cosine_similarity_T += metric(average_of_features_T, features_T)\n",
    "    sum_of_cosine_similarity_F += metric(average_of_features_F, features_F)\n",
    "    print(\"average cosine similarity of T:\", sum_of_cosine_similarity_T / features_T.shape[0])\n",
    "    print(\"average cosine similarity of F:\", sum_of_cosine_similarity_F / features_F.shape[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
